{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Getting started with Pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1o2nXHo5GOFI4sVfMtfKm53cQw6qhX_b1",
      "authorship_tag": "ABX9TyMSH9vSGVFZ/ICs5R+2SZKk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/onyeogulu/Tensorflow-and-Pytorch-Project/blob/main/Getting_started_with_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6C8mlXhahYO",
        "outputId": "d8b9aae1-dc8d-4bd9-9e49-932fb16ad9e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using torch 1.11.0+cu113\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "print(\"Using torch\", torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " if torch.cuda.is_available():\n",
        "   print(\"We have GPU\")\n",
        "else:\n",
        "  print(\"Sorry CPU avaiable\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sveXE_bamwS",
        "outputId": "d68faea5-5a14-412c-acc7-0b0359fb8900"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "-iy6422fam1i",
        "outputId": "85dc2b26-4e7b-4944-9f9a-ea5320fdc872"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.rand(3,226,226)\n",
        "b = a.unsqueeze(0)\n",
        "\n",
        "print(a.shape)\n",
        "print(b.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAiR1XDMam45",
        "outputId": "8b424e3f-6689-4662-de6d-4f84b1789185"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 226, 226])\n",
            "torch.Size([1, 3, 226, 226])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.rand(3).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mh4tRNgkam8R",
        "outputId": "3f212cbe-fc9f-4c67-95dc-afb2ba4377aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch._C import dtype\n",
        "x = torch.empty(2,3, dtype=torch.int32)\n",
        "print(x,dtype)"
      ],
      "metadata": {
        "id": "QVs2o3Nbe_oO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "186093aa-87d7-4f78-8dc3-67bccc42b8ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[600391488,         0,       114],\n",
            "        [       99,       104,        46]], dtype=torch.int32) <class 'torch.dtype'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.rand(3,4)\n",
        "print(y.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBNLAQ0SUoT6",
        "outputId": "0a8f8cfc-451f-4cdd-d617-fe0665bb12d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.09085929 0.11895788 0.10080588 0.901798  ]\n",
            " [0.17093688 0.67761475 0.13768548 0.28997153]\n",
            " [0.410932   0.05376774 0.7948252  0.98124653]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Auto grad Pytorch"
      ],
      "metadata": {
        "id": "ecK0XH97XSst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(3, requires_grad=True)\n",
        "print(x)\n",
        "\n",
        "y = x * 5\n",
        "y = y*y*+2\n",
        "\n",
        "z = y.mean()\n",
        "print(z)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nou9t72IXR6C",
        "outputId": "18254134-e3ed-44e8-a801-23849d13e46a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.6249,  0.4025, -2.1709], requires_grad=True)\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights = torch.ones(4, requires_grad=True)\n",
        "for epoch in range(2):\n",
        "  model_output = (weights*3).sum()\n",
        "  print(model_output)\n",
        "  print(model_output.backward())\n",
        "  print(weights.grad)\n",
        "  weights.grad.zero_()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8IUvYiub-cc",
        "outputId": "a7279cec-f84b-481f-8b8a-f84e721165d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(12., grad_fn=<SumBackward0>)\n",
            "None\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor(12., grad_fn=<SumBackward0>)\n",
            "None\n",
            "tensor([3., 3., 3., 3.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Back Propagation with numpy and torch"
      ],
      "metadata": {
        "id": "PnGPPaP5NPQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "X = np.array([1,2,3,4], dtype = np.float32)\n",
        "Y = np.array([2,4,6,8])\n",
        "w = 0.0\n",
        "\n",
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "# loss = MSE\n",
        "def loss(y, y_predicted):\n",
        "  return ((y_predicted - y)**2).mean()\n",
        "\n",
        "# gradient \n",
        "# MSE = 1/N *(w*x - y)**2\n",
        "# dMSE/dw = 1/N*2w(w*x -y)\n",
        "\n",
        "def gradient(x, y, y_predicted):\n",
        "  return np.dot(2*x,y_predicted - y).mean()\n",
        "\n",
        "print(f'Prediction before training: f(5)={forward(5):.3f}')\n",
        "\n",
        "# training \n",
        "learning_rate = 0.01\n",
        "n_iter = 15\n",
        "\n",
        "for epoch in range(n_iter):\n",
        "  y_pred = forward(X)\n",
        "  print(f'y_predicted:{y_pred}')\n",
        "  #LOSS \n",
        "  l = loss(Y, y_pred)\n",
        "  # gradient\n",
        "  dw = gradient(X,Y,y_pred)\n",
        "  print(f'gradient:{dw}')\n",
        "  # update weight \n",
        "  w -=learning_rate*dw\n",
        "  print(f'weight:{w}')\n",
        "  if epoch % 1 == 0:\n",
        "    print(f'epoch {epoch+1}:w ={w:3f}, loss ={l:.8f}')\n",
        "\n",
        "print(f'Prediction after training:f(5)={forward(5):.3f}')\n"
      ],
      "metadata": {
        "id": "7RjPhQVqlOwf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17550bed-6ee8-49f5-ddcd-8ba57345afcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5)=0.000\n",
            "y_predicted:[0. 0. 0. 0.]\n",
            "gradient:-120.0\n",
            "weight:1.2\n",
            "epoch 1:w =1.200000, loss =30.00000000\n",
            "y_predicted:[1.2       2.4       3.6000001 4.8      ]\n",
            "gradient:-47.99999713897705\n",
            "weight:1.6799999713897704\n",
            "epoch 2:w =1.680000, loss =4.79999943\n",
            "y_predicted:[1.68 3.36 5.04 6.72]\n",
            "gradient:-19.200002431869507\n",
            "weight:1.8719999957084654\n",
            "epoch 3:w =1.872000, loss =0.76800019\n",
            "y_predicted:[1.872 3.744 5.616 7.488]\n",
            "gradient:-7.679999828338623\n",
            "weight:1.9487999939918517\n",
            "epoch 4:w =1.948800, loss =0.12287999\n",
            "y_predicted:[1.9488 3.8976 5.8464 7.7952]\n",
            "gradient:-3.072002649307251\n",
            "weight:1.9795200204849241\n",
            "epoch 5:w =1.979520, loss =0.01966083\n",
            "y_predicted:[1.97952 3.95904 5.93856 7.91808]\n",
            "gradient:-1.2288014888763428\n",
            "weight:1.9918080353736876\n",
            "epoch 6:w =1.991808, loss =0.00314574\n",
            "y_predicted:[1.991808  3.983616  5.9754243 7.967232 ]\n",
            "gradient:-0.4915158748626709\n",
            "weight:1.9967231941223142\n",
            "epoch 7:w =1.996723, loss =0.00050331\n",
            "y_predicted:[1.9967232 3.9934464 5.9901695 7.9868927]\n",
            "gradient:-0.1966094970703125\n",
            "weight:1.9986892890930175\n",
            "epoch 8:w =1.998689, loss =0.00008053\n",
            "y_predicted:[1.9986893 3.9973786 5.996068  7.994757 ]\n",
            "gradient:-0.07864165306091309\n",
            "weight:1.9994757056236265\n",
            "epoch 9:w =1.999476, loss =0.00001288\n",
            "y_predicted:[1.9994757 3.9989514 5.9984274 7.997903 ]\n",
            "gradient:-0.03145551681518555\n",
            "weight:1.9997902607917784\n",
            "epoch 10:w =1.999790, loss =0.00000206\n",
            "y_predicted:[1.9997903 3.9995806 5.999371  7.9991612]\n",
            "gradient:-0.012580633163452148\n",
            "weight:1.9999160671234129\n",
            "epoch 11:w =1.999916, loss =0.00000033\n",
            "y_predicted:[1.9999161 3.9998322 5.999748  7.9996643]\n",
            "gradient:-0.005035400390625\n",
            "weight:1.9999664211273191\n",
            "epoch 12:w =1.999966, loss =0.00000005\n",
            "y_predicted:[1.9999664 3.9999328 5.999899  7.9998655]\n",
            "gradient:-0.002018451690673828\n",
            "weight:1.999986605644226\n",
            "epoch 13:w =1.999987, loss =0.00000001\n",
            "y_predicted:[1.9999866 3.9999733 5.99996   7.9999466]\n",
            "gradient:-0.00080108642578125\n",
            "weight:1.9999946165084836\n",
            "epoch 14:w =1.999995, loss =0.00000000\n",
            "y_predicted:[1.9999946 3.9999893 5.999984  7.9999785]\n",
            "gradient:-0.00032258033752441406\n",
            "weight:1.999997842311859\n",
            "epoch 15:w =1.999998, loss =0.00000000\n",
            "Prediction after training:f(5)=10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "X = torch.tensor([1,2,3,4], dtype = torch.float32)\n",
        "Y = torch.tensor([2,4,6,8],dtype = torch.float32)\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "# loss = MSE\n",
        "def loss(y, y_predicted):\n",
        "  return ((y_predicted - y)**2).mean()\n",
        "\n",
        "# gradient \n",
        "# MSE = 1/N *(w*x - y)**2\n",
        "# dMSE/dw = 1/N*2w(w*x -y)\n",
        "\n",
        "print(f'Prediction before training: f(5)={forward(5):.3f}')\n",
        "\n",
        "# training \n",
        "learning_rate = 0.01\n",
        "n_iter = 20\n",
        "\n",
        "for epoch in range(n_iter):\n",
        "  y_pred = forward(X)\n",
        "  print(f'y_predicted:{y_pred}')\n",
        "  #LOSS \n",
        "  l = loss(Y, y_pred)\n",
        "  # gradient = backward pass\n",
        "  l.backward() #dl/dw\n",
        "  # update weight \n",
        "  with torch.no_grad():\n",
        "    w -=learning_rate*w.grad\n",
        "  # zero gradient \n",
        "  w.grad.zero_()\n",
        "  if epoch % 1 == 0:\n",
        "    print(f'epoch {epoch+1}:w ={w:3f}, loss ={l:.8f}')\n",
        "\n",
        "print(f'Prediction after training:f(5)={forward(5):.3f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2X1cCjDXVYEy",
        "outputId": "8482375f-da67-4f5a-c801-b6ad4a0dd717"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5)=0.000\n",
            "y_predicted:tensor([0., 0., 0., 0.], grad_fn=<MulBackward0>)\n",
            "epoch 1:w =0.300000, loss =30.00000000\n",
            "y_predicted:tensor([0.3000, 0.6000, 0.9000, 1.2000], grad_fn=<MulBackward0>)\n",
            "epoch 2:w =0.555000, loss =21.67499924\n",
            "y_predicted:tensor([0.5550, 1.1100, 1.6650, 2.2200], grad_fn=<MulBackward0>)\n",
            "epoch 3:w =0.771750, loss =15.66018772\n",
            "y_predicted:tensor([0.7717, 1.5435, 2.3152, 3.0870], grad_fn=<MulBackward0>)\n",
            "epoch 4:w =0.955987, loss =11.31448650\n",
            "y_predicted:tensor([0.9560, 1.9120, 2.8680, 3.8239], grad_fn=<MulBackward0>)\n",
            "epoch 5:w =1.112589, loss =8.17471695\n",
            "y_predicted:tensor([1.1126, 2.2252, 3.3378, 4.4504], grad_fn=<MulBackward0>)\n",
            "epoch 6:w =1.245701, loss =5.90623236\n",
            "y_predicted:tensor([1.2457, 2.4914, 3.7371, 4.9828], grad_fn=<MulBackward0>)\n",
            "epoch 7:w =1.358846, loss =4.26725292\n",
            "y_predicted:tensor([1.3588, 2.7177, 4.0765, 5.4354], grad_fn=<MulBackward0>)\n",
            "epoch 8:w =1.455019, loss =3.08308983\n",
            "y_predicted:tensor([1.4550, 2.9100, 4.3651, 5.8201], grad_fn=<MulBackward0>)\n",
            "epoch 9:w =1.536766, loss =2.22753215\n",
            "y_predicted:tensor([1.5368, 3.0735, 4.6103, 6.1471], grad_fn=<MulBackward0>)\n",
            "epoch 10:w =1.606251, loss =1.60939169\n",
            "y_predicted:tensor([1.6063, 3.2125, 4.8188, 6.4250], grad_fn=<MulBackward0>)\n",
            "epoch 11:w =1.665314, loss =1.16278565\n",
            "y_predicted:tensor([1.6653, 3.3306, 4.9959, 6.6613], grad_fn=<MulBackward0>)\n",
            "epoch 12:w =1.715517, loss =0.84011245\n",
            "y_predicted:tensor([1.7155, 3.4310, 5.1465, 6.8621], grad_fn=<MulBackward0>)\n",
            "epoch 13:w =1.758189, loss =0.60698116\n",
            "y_predicted:tensor([1.7582, 3.5164, 5.2746, 7.0328], grad_fn=<MulBackward0>)\n",
            "epoch 14:w =1.794461, loss =0.43854395\n",
            "y_predicted:tensor([1.7945, 3.5889, 5.3834, 7.1778], grad_fn=<MulBackward0>)\n",
            "epoch 15:w =1.825292, loss =0.31684780\n",
            "y_predicted:tensor([1.8253, 3.6506, 5.4759, 7.3012], grad_fn=<MulBackward0>)\n",
            "epoch 16:w =1.851498, loss =0.22892261\n",
            "y_predicted:tensor([1.8515, 3.7030, 5.5545, 7.4060], grad_fn=<MulBackward0>)\n",
            "epoch 17:w =1.873773, loss =0.16539653\n",
            "y_predicted:tensor([1.8738, 3.7475, 5.6213, 7.4951], grad_fn=<MulBackward0>)\n",
            "epoch 18:w =1.892707, loss =0.11949898\n",
            "y_predicted:tensor([1.8927, 3.7854, 5.6781, 7.5708], grad_fn=<MulBackward0>)\n",
            "epoch 19:w =1.908801, loss =0.08633806\n",
            "y_predicted:tensor([1.9088, 3.8176, 5.7264, 7.6352], grad_fn=<MulBackward0>)\n",
            "epoch 20:w =1.922481, loss =0.06237914\n",
            "Prediction after training:f(5)=9.612\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "X = torch.tensor([[1],[2],[3],[4]], dtype = torch.float32)\n",
        "Y = torch.tensor([[2],[4],[6],[8]],dtype = torch.float32)\n",
        "\n",
        "X_test = torch.tensor([5], dtype=torch.float32)\n",
        "n_samples, n_features = X.shape\n",
        "print(n_samples, n_features)\n",
        "input_size = n_features\n",
        "output_size = n_features \n",
        "#model = nn.Linear(input_size, output_size)\n",
        "#print(f'Prediction before training: f(5)={model(X_test).item():.3f}')\n",
        "\n",
        "class LinearRegression(nn.Module):\n",
        "  def __init__(self, input_size,output_size):\n",
        "      super(LinearRegression,self).__init__()\n",
        "      self.lin = nn.Linear(input_size, output_size)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.lin(x)\n",
        "\n",
        "model = LinearRegression(input_size,output_size)\n",
        "# training \n",
        "learning_rate = 0.02\n",
        "n_iter = 40\n",
        "# loss = MSE\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr = learning_rate)\n",
        "for epoch in range(n_iter):\n",
        "  y_pred = model(X)\n",
        "  print(f'y_predicted:{y_pred}')\n",
        "  #LOSS \n",
        "  l = loss(Y, y_pred)\n",
        "  # gradient = backward pass\n",
        "  l.backward() #dl/dw\n",
        "  # update weight \n",
        "  optimizer.step()\n",
        "  # zero gradient \n",
        "  optimizer.zero_grad()\n",
        "  if epoch % 1 == 0:\n",
        "    print(f'epoch {epoch+1}:w ={w:3f}, loss ={l:.8f}')\n",
        "\n",
        "print(f'Prediction after training:f(5)={model(X_test).item():.3f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuQYyYLcZP6e",
        "outputId": "4fee74ef-76f9-48d2-85e7-39cb0721cf40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 1\n",
            "y_predicted:tensor([[1.3890],\n",
            "        [2.1998],\n",
            "        [3.0106],\n",
            "        [3.8214]], grad_fn=<AddmmBackward0>)\n",
            "epoch 1:w =1.996995, loss =7.50261736\n",
            "y_predicted:tensor([[1.7838],\n",
            "        [2.8935],\n",
            "        [4.0032],\n",
            "        [5.1130]], grad_fn=<AddmmBackward0>)\n",
            "epoch 2:w =1.996995, loss =3.39826751\n",
            "y_predicted:tensor([[2.0455],\n",
            "        [3.3549],\n",
            "        [4.6643],\n",
            "        [5.9737]], grad_fn=<AddmmBackward0>)\n",
            "epoch 3:w =1.996995, loss =1.57698131\n",
            "y_predicted:tensor([[2.2187],\n",
            "        [3.6617],\n",
            "        [5.1046],\n",
            "        [6.5476]], grad_fn=<AddmmBackward0>)\n",
            "epoch 4:w =1.996995, loss =0.76833630\n",
            "y_predicted:tensor([[2.3329],\n",
            "        [3.8654],\n",
            "        [5.3979],\n",
            "        [6.9304]], grad_fn=<AddmmBackward0>)\n",
            "epoch 5:w =1.996995, loss =0.40884799\n",
            "y_predicted:tensor([[2.4078],\n",
            "        [4.0006],\n",
            "        [5.5933],\n",
            "        [7.1860]], grad_fn=<AddmmBackward0>)\n",
            "epoch 6:w =1.996995, loss =0.24858794\n",
            "y_predicted:tensor([[2.4566],\n",
            "        [4.0900],\n",
            "        [5.7234],\n",
            "        [7.3568]], grad_fn=<AddmmBackward0>)\n",
            "epoch 7:w =1.996995, loss =0.17670324\n",
            "y_predicted:tensor([[2.4880],\n",
            "        [4.1491],\n",
            "        [5.8101],\n",
            "        [7.4712]], grad_fn=<AddmmBackward0>)\n",
            "epoch 8:w =1.996995, loss =0.14402610\n",
            "y_predicted:tensor([[2.5078],\n",
            "        [4.1879],\n",
            "        [5.8679],\n",
            "        [7.5479]], grad_fn=<AddmmBackward0>)\n",
            "epoch 9:w =1.996995, loss =0.12874876\n",
            "y_predicted:tensor([[2.5199],\n",
            "        [4.2132],\n",
            "        [5.9064],\n",
            "        [7.5997]], grad_fn=<AddmmBackward0>)\n",
            "epoch 10:w =1.996995, loss =0.12119810\n",
            "y_predicted:tensor([[2.5269],\n",
            "        [4.2295],\n",
            "        [5.9321],\n",
            "        [7.6347]], grad_fn=<AddmmBackward0>)\n",
            "epoch 11:w =1.996995, loss =0.11708381\n",
            "y_predicted:tensor([[2.5305],\n",
            "        [4.2398],\n",
            "        [5.9492],\n",
            "        [7.6586]], grad_fn=<AddmmBackward0>)\n",
            "epoch 12:w =1.996995, loss =0.11450288\n",
            "y_predicted:tensor([[2.5317],\n",
            "        [4.2462],\n",
            "        [5.9607],\n",
            "        [7.6752]], grad_fn=<AddmmBackward0>)\n",
            "epoch 13:w =1.996995, loss =0.11261101\n",
            "y_predicted:tensor([[2.5315],\n",
            "        [4.2499],\n",
            "        [5.9683],\n",
            "        [7.6867]], grad_fn=<AddmmBackward0>)\n",
            "epoch 14:w =1.996995, loss =0.11103373\n",
            "y_predicted:tensor([[2.5303],\n",
            "        [4.2519],\n",
            "        [5.9735],\n",
            "        [7.6950]], grad_fn=<AddmmBackward0>)\n",
            "epoch 15:w =1.996995, loss =0.10960477\n",
            "y_predicted:tensor([[2.5285],\n",
            "        [4.2527],\n",
            "        [5.9769],\n",
            "        [7.7011]], grad_fn=<AddmmBackward0>)\n",
            "epoch 16:w =1.996995, loss =0.10825034\n",
            "y_predicted:tensor([[2.5262],\n",
            "        [4.2527],\n",
            "        [5.9793],\n",
            "        [7.7058]], grad_fn=<AddmmBackward0>)\n",
            "epoch 17:w =1.996995, loss =0.10693759\n",
            "y_predicted:tensor([[2.5236],\n",
            "        [4.2522],\n",
            "        [5.9808],\n",
            "        [7.7094]], grad_fn=<AddmmBackward0>)\n",
            "epoch 18:w =1.996995, loss =0.10565177\n",
            "y_predicted:tensor([[2.5209],\n",
            "        [4.2514],\n",
            "        [5.9819],\n",
            "        [7.7124]], grad_fn=<AddmmBackward0>)\n",
            "epoch 19:w =1.996995, loss =0.10438623\n",
            "y_predicted:tensor([[2.5180],\n",
            "        [4.2504],\n",
            "        [5.9827],\n",
            "        [7.7150]], grad_fn=<AddmmBackward0>)\n",
            "epoch 20:w =1.996995, loss =0.10313819\n",
            "y_predicted:tensor([[2.5151],\n",
            "        [4.2492],\n",
            "        [5.9832],\n",
            "        [7.7173]], grad_fn=<AddmmBackward0>)\n",
            "epoch 21:w =1.996995, loss =0.10190593\n",
            "y_predicted:tensor([[2.5121],\n",
            "        [4.2479],\n",
            "        [5.9836],\n",
            "        [7.7193]], grad_fn=<AddmmBackward0>)\n",
            "epoch 22:w =1.996995, loss =0.10068893\n",
            "y_predicted:tensor([[2.5091],\n",
            "        [4.2465],\n",
            "        [5.9839],\n",
            "        [7.7213]], grad_fn=<AddmmBackward0>)\n",
            "epoch 23:w =1.996995, loss =0.09948658\n",
            "y_predicted:tensor([[2.5061],\n",
            "        [4.2451],\n",
            "        [5.9841],\n",
            "        [7.7231]], grad_fn=<AddmmBackward0>)\n",
            "epoch 24:w =1.996995, loss =0.09829863\n",
            "y_predicted:tensor([[2.5031],\n",
            "        [4.2437],\n",
            "        [5.9843],\n",
            "        [7.7249]], grad_fn=<AddmmBackward0>)\n",
            "epoch 25:w =1.996995, loss =0.09712490\n",
            "y_predicted:tensor([[2.5002],\n",
            "        [4.2423],\n",
            "        [5.9844],\n",
            "        [7.7266]], grad_fn=<AddmmBackward0>)\n",
            "epoch 26:w =1.996995, loss =0.09596528\n",
            "y_predicted:tensor([[2.4972],\n",
            "        [4.2409],\n",
            "        [5.9846],\n",
            "        [7.7283]], grad_fn=<AddmmBackward0>)\n",
            "epoch 27:w =1.996995, loss =0.09481943\n",
            "y_predicted:tensor([[2.4942],\n",
            "        [4.2395],\n",
            "        [5.9847],\n",
            "        [7.7299]], grad_fn=<AddmmBackward0>)\n",
            "epoch 28:w =1.996995, loss =0.09368733\n",
            "y_predicted:tensor([[2.4913],\n",
            "        [4.2380],\n",
            "        [5.9848],\n",
            "        [7.7316]], grad_fn=<AddmmBackward0>)\n",
            "epoch 29:w =1.996995, loss =0.09256873\n",
            "y_predicted:tensor([[2.4883],\n",
            "        [4.2366],\n",
            "        [5.9849],\n",
            "        [7.7332]], grad_fn=<AddmmBackward0>)\n",
            "epoch 30:w =1.996995, loss =0.09146346\n",
            "y_predicted:tensor([[2.4854],\n",
            "        [4.2352],\n",
            "        [5.9850],\n",
            "        [7.7348]], grad_fn=<AddmmBackward0>)\n",
            "epoch 31:w =1.996995, loss =0.09037144\n",
            "y_predicted:tensor([[2.4825],\n",
            "        [4.2338],\n",
            "        [5.9851],\n",
            "        [7.7364]], grad_fn=<AddmmBackward0>)\n",
            "epoch 32:w =1.996995, loss =0.08929244\n",
            "y_predicted:tensor([[2.4796],\n",
            "        [4.2324],\n",
            "        [5.9852],\n",
            "        [7.7380]], grad_fn=<AddmmBackward0>)\n",
            "epoch 33:w =1.996995, loss =0.08822633\n",
            "y_predicted:tensor([[2.4767],\n",
            "        [4.2310],\n",
            "        [5.9853],\n",
            "        [7.7396]], grad_fn=<AddmmBackward0>)\n",
            "epoch 34:w =1.996995, loss =0.08717284\n",
            "y_predicted:tensor([[2.4739],\n",
            "        [4.2296],\n",
            "        [5.9854],\n",
            "        [7.7411]], grad_fn=<AddmmBackward0>)\n",
            "epoch 35:w =1.996995, loss =0.08613207\n",
            "y_predicted:tensor([[2.4710],\n",
            "        [4.2283],\n",
            "        [5.9855],\n",
            "        [7.7427]], grad_fn=<AddmmBackward0>)\n",
            "epoch 36:w =1.996995, loss =0.08510371\n",
            "y_predicted:tensor([[2.4682],\n",
            "        [4.2269],\n",
            "        [5.9855],\n",
            "        [7.7442]], grad_fn=<AddmmBackward0>)\n",
            "epoch 37:w =1.996995, loss =0.08408757\n",
            "y_predicted:tensor([[2.4654],\n",
            "        [4.2255],\n",
            "        [5.9856],\n",
            "        [7.7457]], grad_fn=<AddmmBackward0>)\n",
            "epoch 38:w =1.996995, loss =0.08308353\n",
            "y_predicted:tensor([[2.4626],\n",
            "        [4.2242],\n",
            "        [5.9857],\n",
            "        [7.7473]], grad_fn=<AddmmBackward0>)\n",
            "epoch 39:w =1.996995, loss =0.08209158\n",
            "y_predicted:tensor([[2.4599],\n",
            "        [4.2228],\n",
            "        [5.9858],\n",
            "        [7.7488]], grad_fn=<AddmmBackward0>)\n",
            "epoch 40:w =1.996995, loss =0.08111139\n",
            "Prediction after training:f(5)=9.515\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# prepare data \n",
        "X_numpy, y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=1)\n",
        "\n",
        "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
        "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
        "\n",
        "y = y.view(y.shape[0],1)\n",
        "#print(y)\n",
        "n_samples , n_features = X.shape\n",
        "input_size = n_features\n",
        "output_size = 1\n",
        "learning_rate = 0.01\n",
        "print(X.shape)\n",
        "# model \n",
        "model = nn.Linear(input_size, output_size)\n",
        "\n",
        "#loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "  # predict\n",
        "  y_pred = model(X)\n",
        "  # loss \n",
        "  loss = criterion(y_pred, y)\n",
        "  #back propagation\n",
        "  loss.backward()\n",
        "  # update weight \n",
        "  optimizer.step()\n",
        "  # empty auto grad\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if (epoch+1)%10 ==0:\n",
        "    print(f'epoch:{epoch+1}, loss:{loss.item():4f}')\n",
        "\n",
        "# plot \n",
        "predicted = model(X).detach().numpy()\n",
        "\n",
        "plt.plot(X_numpy, y_numpy, 'ro')\n",
        "plt.plot(X_numpy, predicted, 'b')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "793zFeZ4n1m9",
        "outputId": "8202a156-5cdb-4d0f-b630-a6c7de85d072"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100, 1])\n",
            "epoch:10, loss:4365.834473\n",
            "epoch:20, loss:3255.746338\n",
            "epoch:30, loss:2453.188477\n",
            "epoch:40, loss:1872.312378\n",
            "epoch:50, loss:1451.446533\n",
            "epoch:60, loss:1146.218384\n",
            "epoch:70, loss:924.658203\n",
            "epoch:80, loss:763.698059\n",
            "epoch:90, loss:646.674500\n",
            "epoch:100, loss:561.534851\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression"
      ],
      "metadata": {
        "id": "azIgYeL-0cC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split \n",
        "\n",
        "bc = datasets.load_breast_cancer()\n",
        "X, y = bc.data, bc.target\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 1234)\n",
        "\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.fit_transform(X_test)\n",
        "\n",
        "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
        "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
        "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
        "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
        "\n",
        "y_train = y_train.view(y_train.shape[0],1)\n",
        "y_test = y_test.view(y_test.shape[0],1)\n",
        "\n",
        "\n",
        "class LogisticRegression(nn.Module):\n",
        "  def __init__(self, n_input_features):\n",
        "      super(LogisticRegression, self).__init__()\n",
        "      self.linear = nn.Linear(n_input_features, 1)\n",
        "  def forward(self, x):\n",
        "    y_pred = torch.sigmoid(self.linear(x))\n",
        "    return y_pred\n",
        "model = LogisticRegression(n_features)\n",
        "# loss and optimzer \n",
        "learning_rate = 0.01\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(),learning_rate)\n",
        "# training loop \n",
        "num_epochs = 200\n",
        "for epoch in range(num_epochs):\n",
        "  y_predicted = model(X_train)\n",
        "  loss = criterion(y_predicted, y_train)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if (epoch+1)%10 ==0:\n",
        "    print(f'epoch:{epoch+1}, loss ={loss.item():.4f}')\n",
        "\n",
        "with torch.no_grad():\n",
        "   y_predicted = model(X_test)\n",
        "   y_pred_cls = y_predicted.round()\n",
        "   acc = y_pred_cls.eq(y_test).sum()/float(y_test.shape[0])\n",
        "   print(f'accuracy ={acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SuzqxAl0hEA",
        "outputId": "96c24df9-9e3c-4315-db0a-dfb7abe1bdc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:10, loss =0.5503\n",
            "epoch:20, loss =0.4507\n",
            "epoch:30, loss =0.3861\n",
            "epoch:40, loss =0.3416\n",
            "epoch:50, loss =0.3093\n",
            "epoch:60, loss =0.2849\n",
            "epoch:70, loss =0.2657\n",
            "epoch:80, loss =0.2502\n",
            "epoch:90, loss =0.2374\n",
            "epoch:100, loss =0.2266\n",
            "epoch:110, loss =0.2174\n",
            "epoch:120, loss =0.2093\n",
            "epoch:130, loss =0.2022\n",
            "epoch:140, loss =0.1959\n",
            "epoch:150, loss =0.1902\n",
            "epoch:160, loss =0.1850\n",
            "epoch:170, loss =0.1803\n",
            "epoch:180, loss =0.1760\n",
            "epoch:190, loss =0.1720\n",
            "epoch:200, loss =0.1684\n",
            "accuracy =0.9298\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision \n",
        "from torch.utils.data import Dataset, DataLoader \n",
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "# datset class\n",
        "class WineDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    xy = np.loadtxt('/content/drive/MyDrive/ColabNotebooks/Dataset/wine.csv', delimiter=\",\", dtype =np.float32,skiprows=1)\n",
        "    self.x = torch.from_numpy(xy[:,1:])\n",
        "    self.y = torch.from_numpy(xy[:,[0]])\n",
        "    self.n_samples = xy.shape[0]\n",
        "  def __getitem__(self, index):\n",
        "    return self.x[index], self.y[index]\n",
        "  def __len__(self):\n",
        "    return self.n_samples\n",
        "#instantiate datset class \n",
        "dataset = WineDataset()\n",
        "\n",
        "# load dataset\n",
        "dataloader = DataLoader(dataset=dataset,batch_size=5, shuffle=True, num_workers=2)\n",
        "\n",
        "dataiter = iter(dataloader)\n",
        "data = dataiter.next()\n",
        "features, label = data\n",
        "print(features, label)\n",
        "\n",
        "num_epochs = 2\n",
        "total_sample = len(dataset)\n",
        "n_iteration = math.ceil(total_sample/5)\n",
        "print(total_sample, n_iteration)\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (inputs, labels) in enumerate(dataloader):\n",
        "    if (i+1)%6 ==0:\n",
        "      print(f'epoch{epoch+1}/{num_epochs}, step {i+1}/{n_iteration}, inputs{inputs.shape}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1lbOVZg_zPM",
        "outputId": "93fc25df-e85c-4a35-fb97-73b18ce036c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.1650e+01, 1.6700e+00, 2.6200e+00, 2.6000e+01, 8.8000e+01, 1.9200e+00,\n",
            "         1.6100e+00, 4.0000e-01, 1.3400e+00, 2.6000e+00, 1.3600e+00, 3.2100e+00,\n",
            "         5.6200e+02],\n",
            "        [1.2040e+01, 4.3000e+00, 2.3800e+00, 2.2000e+01, 8.0000e+01, 2.1000e+00,\n",
            "         1.7500e+00, 4.2000e-01, 1.3500e+00, 2.6000e+00, 7.9000e-01, 2.5700e+00,\n",
            "         5.8000e+02],\n",
            "        [1.3940e+01, 1.7300e+00, 2.2700e+00, 1.7400e+01, 1.0800e+02, 2.8800e+00,\n",
            "         3.5400e+00, 3.2000e-01, 2.0800e+00, 8.9000e+00, 1.1200e+00, 3.1000e+00,\n",
            "         1.2600e+03],\n",
            "        [1.2850e+01, 1.6000e+00, 2.5200e+00, 1.7800e+01, 9.5000e+01, 2.4800e+00,\n",
            "         2.3700e+00, 2.6000e-01, 1.4600e+00, 3.9300e+00, 1.0900e+00, 3.6300e+00,\n",
            "         1.0150e+03],\n",
            "        [1.3030e+01, 9.0000e-01, 1.7100e+00, 1.6000e+01, 8.6000e+01, 1.9500e+00,\n",
            "         2.0300e+00, 2.4000e-01, 1.4600e+00, 4.6000e+00, 1.1900e+00, 2.4800e+00,\n",
            "         3.9200e+02]]) tensor([[2.],\n",
            "        [2.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [2.]])\n",
            "178 36\n",
            "epoch1/2, step 6/36, inputstorch.Size([5, 13])\n",
            "epoch1/2, step 12/36, inputstorch.Size([5, 13])\n",
            "epoch1/2, step 18/36, inputstorch.Size([5, 13])\n",
            "epoch1/2, step 24/36, inputstorch.Size([5, 13])\n",
            "epoch1/2, step 30/36, inputstorch.Size([5, 13])\n",
            "epoch1/2, step 36/36, inputstorch.Size([3, 13])\n",
            "epoch2/2, step 6/36, inputstorch.Size([5, 13])\n",
            "epoch2/2, step 12/36, inputstorch.Size([5, 13])\n",
            "epoch2/2, step 18/36, inputstorch.Size([5, 13])\n",
            "epoch2/2, step 24/36, inputstorch.Size([5, 13])\n",
            "epoch2/2, step 30/36, inputstorch.Size([5, 13])\n",
            "epoch2/2, step 36/36, inputstorch.Size([3, 13])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transforms "
      ],
      "metadata": {
        "id": "vGAls54V6i9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision \n",
        "from torch.utils.data import Dataset, DataLoader \n",
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "class WineDataset(Dataset):\n",
        "  def __init__(self, transform=None):\n",
        "    xy = np.loadtxt('/content/drive/MyDrive/ColabNotebooks/Dataset/wine.csv', delimiter=\",\", dtype =np.float32,skiprows=1)\n",
        "    self.x = xy[:,1:]\n",
        "    self.y = xy[:,[0]]\n",
        "    self.transform = transform\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    sample = self.x[index], self.y[index]\n",
        "    if self.transform:\n",
        "      sample = self.transform(sample)\n",
        "    return sample \n",
        "  def __len__(self):\n",
        "    return self.n_samples\n",
        "\n",
        "class ToTensor:\n",
        "  def __call__(self, sample):\n",
        "    inputs, targets = sample \n",
        "    return torch.from_numpy(inputs), torch.from_numpy(targets)\n",
        "\n",
        "class Multi:\n",
        "  def __init__(self,factor):\n",
        "    self.factor = factor\n",
        "\n",
        "  def __call__(self, sample):\n",
        "    input, target = sample \n",
        "    input = input *self.factor\n",
        "    return input, target\n",
        "  \n",
        "\n",
        "#instantiate datset class \n",
        "dataset = WineDataset(transform=ToTensor())\n",
        "firstdata = dataset[5]\n",
        "features, labels = firstdata\n",
        "print(features, labels)\n",
        "\n",
        "composed = torchvision.transforms.Compose([ToTensor(), Multi(3)])\n",
        "\n",
        "#instantiate datset class \n",
        "dataset = WineDataset(transform=composed)\n",
        "firstdata = dataset[5]\n",
        "features, labels = firstdata\n",
        "print(features, labels)"
      ],
      "metadata": {
        "id": "XF6afT1GIJeY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be144c86-9bf7-4a6e-ac2e-f82426b4b4bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.4200e+01, 1.7600e+00, 2.4500e+00, 1.5200e+01, 1.1200e+02, 3.2700e+00,\n",
            "        3.3900e+00, 3.4000e-01, 1.9700e+00, 6.7500e+00, 1.0500e+00, 2.8500e+00,\n",
            "        1.4500e+03]) tensor([1.])\n",
            "tensor([4.2600e+01, 5.2800e+00, 7.3500e+00, 4.5600e+01, 3.3600e+02, 9.8100e+00,\n",
            "        1.0170e+01, 1.0200e+00, 5.9100e+00, 2.0250e+01, 3.1500e+00, 8.5500e+00,\n",
            "        4.3500e+03]) tensor([1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Download training data from open datasets.\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)\n",
        "\n",
        "# define hyperparameters \n",
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 30\n",
        "\n",
        "# Initialize the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "# optimasation function \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# traing loop \n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      # Compute prediction and loss\n",
        "      pred = model(X)\n",
        "      loss = loss_fn(pred, y)\n",
        "\n",
        "      # Backpropagation\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if batch % 100 == 0:\n",
        "        loss, current = loss.item(), batch * len(X)\n",
        "        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "# testing loop \n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  test_loss, correct = 0, 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      pred = model(X)\n",
        "      test_loss += loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "# train the model \n",
        "epochs = 20\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")\n"
      ],
      "metadata": {
        "id": "hF5epTtgIJhO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3e47673-27db-4f49-acbd-2a3b5da2f795"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN in pytorch"
      ],
      "metadata": {
        "id": "PwLymyZ7Zr1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules.activation import ReLU\n",
        "from torch.nn.modules.linear import Linear\n",
        "from torch.nn.modules import flatten\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Download training data from open datasets.\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "dataiter = iter(train_dataloader)\n",
        "images, labels = dataiter.next()\n",
        "print(type(images))\n",
        "print(images.shape)\n",
        "print(labels.shape)\n",
        "\n",
        "# Define model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(NeuralNetwork, self).__init__()\n",
        "      self.conv1 = nn.Conv2d(1, 20, 4)\n",
        "      self.conv2 = nn.Conv2d(20, 15, 4)\n",
        "      self.fc1 = nn.Linear(15*4*4, 200)\n",
        "      self.fc2 = nn.Linear(200, 100)\n",
        "      self.fc3 = nn.Linear(100,10)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = F.max_pool2d(F.relu(self.conv1(x)),(2,2))# first layer\n",
        "      x = F.max_pool2d(F.relu(self.conv2(x)),2) # second layer \n",
        "      x = torch.flatten(x,1) # flatten\n",
        "      x = F.relu(self.fc1(x)) #fully connected layer\n",
        "      x = F.relu(self.fc2(x)) #fully connected layer \n",
        "      x = F.relu(self.fc3(x)) #fully connected layer\n",
        "      return x\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)\n",
        "\n",
        "# define hyperparameters \n",
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 30\n",
        "\n",
        "# Initialize the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "# optimasation function \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# traing loop \n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      # Compute prediction and loss\n",
        "      pred = model(X)\n",
        "      loss = loss_fn(pred, y)\n",
        "\n",
        "      # Backpropagation\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if batch % 100 == 0:\n",
        "        loss, current = loss.item(), batch * len(X)\n",
        "        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "# testing loop \n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  test_loss, correct = 0, 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      pred = model(X)\n",
        "      test_loss += loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "# train the model \n",
        "epochs = 20\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "id": "2OAgq9-PIJm_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9184e7df-b7d0-4730-d292-15c692bded69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "NeuralNetwork(\n",
            "  (conv1): Conv2d(1, 20, kernel_size=(4, 4), stride=(1, 1))\n",
            "  (conv2): Conv2d(20, 15, kernel_size=(4, 4), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=240, out_features=200, bias=True)\n",
            "  (fc2): Linear(in_features=200, out_features=100, bias=True)\n",
            "  (fc3): Linear(in_features=100, out_features=10, bias=True)\n",
            ")\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.302636  [    0/60000]\n",
            "loss: 2.301915  [ 6400/60000]\n",
            "loss: 2.299515  [12800/60000]\n",
            "loss: 2.297239  [19200/60000]\n",
            "loss: 2.308193  [25600/60000]\n",
            "loss: 2.302568  [32000/60000]\n",
            "loss: 2.299686  [38400/60000]\n",
            "loss: 2.300413  [44800/60000]\n",
            "loss: 2.296213  [51200/60000]\n",
            "loss: 2.302828  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 10.0%, Avg loss: 2.299805 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.299209  [    0/60000]\n",
            "loss: 2.299528  [ 6400/60000]\n",
            "loss: 2.296225  [12800/60000]\n",
            "loss: 2.294304  [19200/60000]\n",
            "loss: 2.305394  [25600/60000]\n",
            "loss: 2.298453  [32000/60000]\n",
            "loss: 2.296969  [38400/60000]\n",
            "loss: 2.296754  [44800/60000]\n",
            "loss: 2.292979  [51200/60000]\n",
            "loss: 2.298811  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 10.0%, Avg loss: 2.296496 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 2.296281  [    0/60000]\n",
            "loss: 2.297224  [ 6400/60000]\n",
            "loss: 2.293007  [12800/60000]\n",
            "loss: 2.291343  [19200/60000]\n",
            "loss: 2.302623  [25600/60000]\n",
            "loss: 2.294139  [32000/60000]\n",
            "loss: 2.293706  [38400/60000]\n",
            "loss: 2.292715  [44800/60000]\n",
            "loss: 2.289266  [51200/60000]\n",
            "loss: 2.294031  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 10.0%, Avg loss: 2.292386 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 2.292740  [    0/60000]\n",
            "loss: 2.294187  [ 6400/60000]\n",
            "loss: 2.288789  [12800/60000]\n",
            "loss: 2.287378  [19200/60000]\n",
            "loss: 2.298786  [25600/60000]\n",
            "loss: 2.288115  [32000/60000]\n",
            "loss: 2.289113  [38400/60000]\n",
            "loss: 2.287025  [44800/60000]\n",
            "loss: 2.283334  [51200/60000]\n",
            "loss: 2.287013  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 13.4%, Avg loss: 2.286102 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 2.287549  [    0/60000]\n",
            "loss: 2.289393  [ 6400/60000]\n",
            "loss: 2.282578  [12800/60000]\n",
            "loss: 2.280580  [19200/60000]\n",
            "loss: 2.292687  [25600/60000]\n",
            "loss: 2.279291  [32000/60000]\n",
            "loss: 2.280755  [38400/60000]\n",
            "loss: 2.278227  [44800/60000]\n",
            "loss: 2.273711  [51200/60000]\n",
            "loss: 2.274285  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 29.9%, Avg loss: 2.275976 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 2.279827  [    0/60000]\n",
            "loss: 2.282015  [ 6400/60000]\n",
            "loss: 2.272603  [12800/60000]\n",
            "loss: 2.268738  [19200/60000]\n",
            "loss: 2.282852  [25600/60000]\n",
            "loss: 2.263928  [32000/60000]\n",
            "loss: 2.265543  [38400/60000]\n",
            "loss: 2.261531  [44800/60000]\n",
            "loss: 2.255382  [51200/60000]\n",
            "loss: 2.250976  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 30.3%, Avg loss: 2.256301 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 2.264535  [    0/60000]\n",
            "loss: 2.267400  [ 6400/60000]\n",
            "loss: 2.251836  [12800/60000]\n",
            "loss: 2.243757  [19200/60000]\n",
            "loss: 2.263413  [25600/60000]\n",
            "loss: 2.231689  [32000/60000]\n",
            "loss: 2.232886  [38400/60000]\n",
            "loss: 2.223936  [44800/60000]\n",
            "loss: 2.213022  [51200/60000]\n",
            "loss: 2.199262  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 31.9%, Avg loss: 2.209755 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 2.227872  [    0/60000]\n",
            "loss: 2.230581  [ 6400/60000]\n",
            "loss: 2.198127  [12800/60000]\n",
            "loss: 2.177874  [19200/60000]\n",
            "loss: 2.211994  [25600/60000]\n",
            "loss: 2.143145  [32000/60000]\n",
            "loss: 2.137910  [38400/60000]\n",
            "loss: 2.110588  [44800/60000]\n",
            "loss: 2.081130  [51200/60000]\n",
            "loss: 2.046673  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 37.0%, Avg loss: 2.067041 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 2.111403  [    0/60000]\n",
            "loss: 2.108211  [ 6400/60000]\n",
            "loss: 2.031861  [12800/60000]\n",
            "loss: 1.974702  [19200/60000]\n",
            "loss: 2.076225  [25600/60000]\n",
            "loss: 1.906445  [32000/60000]\n",
            "loss: 1.877792  [38400/60000]\n",
            "loss: 1.828803  [44800/60000]\n",
            "loss: 1.797168  [51200/60000]\n",
            "loss: 1.764959  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 45.1%, Avg loss: 1.820108 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.914438  [    0/60000]\n",
            "loss: 1.912984  [ 6400/60000]\n",
            "loss: 1.793085  [12800/60000]\n",
            "loss: 1.712290  [19200/60000]\n",
            "loss: 1.853386  [25600/60000]\n",
            "loss: 1.580915  [32000/60000]\n",
            "loss: 1.581838  [38400/60000]\n",
            "loss: 1.441671  [44800/60000]\n",
            "loss: 1.563428  [51200/60000]\n",
            "loss: 1.458969  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 56.3%, Avg loss: 1.520633 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 1.674262  [    0/60000]\n",
            "loss: 1.618110  [ 6400/60000]\n",
            "loss: 1.435005  [12800/60000]\n",
            "loss: 1.483609  [19200/60000]\n",
            "loss: 1.589630  [25600/60000]\n",
            "loss: 1.334965  [32000/60000]\n",
            "loss: 1.383152  [38400/60000]\n",
            "loss: 1.226256  [44800/60000]\n",
            "loss: 1.400940  [51200/60000]\n",
            "loss: 1.300254  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 58.4%, Avg loss: 1.382389 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 1.553210  [    0/60000]\n",
            "loss: 1.490873  [ 6400/60000]\n",
            "loss: 1.295765  [12800/60000]\n",
            "loss: 1.379571  [19200/60000]\n",
            "loss: 1.486370  [25600/60000]\n",
            "loss: 1.237836  [32000/60000]\n",
            "loss: 1.291199  [38400/60000]\n",
            "loss: 1.126805  [44800/60000]\n",
            "loss: 1.302868  [51200/60000]\n",
            "loss: 1.232059  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 59.6%, Avg loss: 1.313370 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 1.480501  [    0/60000]\n",
            "loss: 1.423218  [ 6400/60000]\n",
            "loss: 1.219125  [12800/60000]\n",
            "loss: 1.324036  [19200/60000]\n",
            "loss: 1.437139  [25600/60000]\n",
            "loss: 1.179128  [32000/60000]\n",
            "loss: 1.238591  [38400/60000]\n",
            "loss: 1.070490  [44800/60000]\n",
            "loss: 1.247504  [51200/60000]\n",
            "loss: 1.202925  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 60.6%, Avg loss: 1.272906 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 1.437521  [    0/60000]\n",
            "loss: 1.387882  [ 6400/60000]\n",
            "loss: 1.167212  [12800/60000]\n",
            "loss: 1.291865  [19200/60000]\n",
            "loss: 1.407176  [25600/60000]\n",
            "loss: 1.141033  [32000/60000]\n",
            "loss: 1.199117  [38400/60000]\n",
            "loss: 1.035620  [44800/60000]\n",
            "loss: 1.215191  [51200/60000]\n",
            "loss: 1.188160  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 61.4%, Avg loss: 1.244461 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 1.404212  [    0/60000]\n",
            "loss: 1.365710  [ 6400/60000]\n",
            "loss: 1.132969  [12800/60000]\n",
            "loss: 1.266940  [19200/60000]\n",
            "loss: 1.380864  [25600/60000]\n",
            "loss: 1.113247  [32000/60000]\n",
            "loss: 1.168084  [38400/60000]\n",
            "loss: 1.013442  [44800/60000]\n",
            "loss: 1.193670  [51200/60000]\n",
            "loss: 1.178864  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 62.2%, Avg loss: 1.223047 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 1.379005  [    0/60000]\n",
            "loss: 1.350262  [ 6400/60000]\n",
            "loss: 1.103489  [12800/60000]\n",
            "loss: 1.245397  [19200/60000]\n",
            "loss: 1.359535  [25600/60000]\n",
            "loss: 1.092104  [32000/60000]\n",
            "loss: 1.142555  [38400/60000]\n",
            "loss: 0.999526  [44800/60000]\n",
            "loss: 1.178084  [51200/60000]\n",
            "loss: 1.172107  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 62.7%, Avg loss: 1.205500 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 1.358751  [    0/60000]\n",
            "loss: 1.340692  [ 6400/60000]\n",
            "loss: 1.079574  [12800/60000]\n",
            "loss: 1.219850  [19200/60000]\n",
            "loss: 1.340788  [25600/60000]\n",
            "loss: 1.074268  [32000/60000]\n",
            "loss: 1.121431  [38400/60000]\n",
            "loss: 0.989323  [44800/60000]\n",
            "loss: 1.166435  [51200/60000]\n",
            "loss: 1.166335  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 63.4%, Avg loss: 1.190756 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 1.342691  [    0/60000]\n",
            "loss: 1.332904  [ 6400/60000]\n",
            "loss: 1.060167  [12800/60000]\n",
            "loss: 1.197673  [19200/60000]\n",
            "loss: 1.323094  [25600/60000]\n",
            "loss: 1.057148  [32000/60000]\n",
            "loss: 1.102306  [38400/60000]\n",
            "loss: 0.981117  [44800/60000]\n",
            "loss: 1.156517  [51200/60000]\n",
            "loss: 1.161011  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 64.0%, Avg loss: 1.178032 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 1.327589  [    0/60000]\n",
            "loss: 1.325671  [ 6400/60000]\n",
            "loss: 1.043358  [12800/60000]\n",
            "loss: 1.179034  [19200/60000]\n",
            "loss: 1.310083  [25600/60000]\n",
            "loss: 1.042076  [32000/60000]\n",
            "loss: 1.085364  [38400/60000]\n",
            "loss: 0.971792  [44800/60000]\n",
            "loss: 1.149482  [51200/60000]\n",
            "loss: 1.155683  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 64.4%, Avg loss: 1.166504 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 1.313260  [    0/60000]\n",
            "loss: 1.317678  [ 6400/60000]\n",
            "loss: 1.025548  [12800/60000]\n",
            "loss: 1.162397  [19200/60000]\n",
            "loss: 1.296409  [25600/60000]\n",
            "loss: 1.029191  [32000/60000]\n",
            "loss: 1.071084  [38400/60000]\n",
            "loss: 0.964525  [44800/60000]\n",
            "loss: 1.142348  [51200/60000]\n",
            "loss: 1.150997  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 64.7%, Avg loss: 1.156029 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "SJMQ-g8gIJqj",
        "outputId": "891b0c72-a79a-43c6-b660-6e8c07aa4b65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-5348af5b9632>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'mean' and 'std'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZradF_79p9-7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}